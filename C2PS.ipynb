{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "QxNOtXqrUpWQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "052cd3f0-1230-4f72-e59a-e489454b403d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.30.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader, random_split, SubsetRandomSampler\n",
        "from transformers import Trainer, AutoTokenizer, AutoModel, AutoConfig\n",
        "from sklearn.model_selection import KFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "RxKIjUbIiEyo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "cpp bert의 last hidden state size: torch.Size([1, 7, 768])\n",
        "gpt의 input size: [1, n]"
      ],
      "metadata": {
        "id": "ut6sk-b-o_GN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class CppDataset(Dataset):\n",
        "  def __init__(self):\n",
        "    # initializing - read text\n",
        "    code_df = pd.read_tsv(\"spoc-train.tsv\")\n",
        "    self.x = code_df.iloc[:, 2].values\n",
        "    self.y = code_df.iloc[:, 1].values\n",
        "\n",
        "    self.tokenized = [self.tokenize_code(code) for code in self.x] #code를 tokenize한 결과를 저장\n",
        "\n",
        "  def __getitem__(self, index):\n",
        "    inputs = {\n",
        "        \"input_ids\" : torch.tensor(self.tokenized[index][\"input_ids\"], dtype = torch.long, device = device),\n",
        "        \"attention_mask\" : torch.tensor(self.tokenized[index][\"attention_mask\"], dtype = torch.long, device = device)\n",
        "    } #tokenized index를 tensor로 변환\n",
        "    #attention..도 적용 > 이거 꼭 해야하는건가요? cpp > ps 대응할 때 필요한 것 같아보이긴합니다만 잘모르겠어요 흑흑\n",
        "    targets = torch.tensor(self.y[index], dtype=torch.float32)#Pseudo code 표현\n",
        "    return inputs, targets\n",
        "\n",
        "# tokenizing code\n",
        "def tokenize_code(code):\n",
        "  tokenizer = BertTokenizer.from_pretrained(\"bert-base-cpp\") #load cpp BERT tokenizer\n",
        "  tokenized_code = []\n",
        "\n",
        "  return tokenized_code"
      ],
      "metadata": {
        "id": "N9PE2XG_ipWw"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class C2PSClass(torch.nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.encoder = BertModel.from_pretrained(\"bert-base-cpp\")\n",
        "    self.decoder = GPT2Model.from_pretrained(\"gpt2\")\n",
        "    self.pooling = torch.nn.MaxPool2d(kernel_size=(1,2)) #2d MaxPooling layer\n",
        "\n",
        "  def forward(self, inputs):\n",
        "    encoder_outputs = self.encoder(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "    decoder_outputs = self.decoder(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"])\n",
        "    pooled_outputs = self.pooling(encoder_outputs[0]) #MaxPooling 적용\n",
        "    outputs = pooled_outputs.squeeze(2).transpose(1, 2).squeeze(2) #1D tensor\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "m5y83DWBkJjD"
      },
      "execution_count": 19,
      "outputs": []
    }
  ]
}